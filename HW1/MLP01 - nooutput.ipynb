{
 "cells": [
  {
   "source": [
    "<center><h3>ภาคผนวก</h3></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T16:19:19.570916Z",
     "start_time": "2019-10-20T16:19:19.566880Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T16:19:20.680209Z",
     "start_time": "2019-10-20T16:19:20.658254Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_txt(path):\n",
    "    f=open(path, \"r\")\n",
    "    if(path[-3:] == 'txt'):\n",
    "        contents =f.readlines()\n",
    "\n",
    "        dataset = np.zeros((len(contents)-2,len(contents[2].split('\\t'))-1))\n",
    "        label = np.zeros((len(contents)-2))\n",
    "\n",
    "        for i in range(len(contents)-2):\n",
    "            x = contents[i+2].split(\"\\t\")\n",
    "            for j in range(len(x)):\n",
    "                if j != len(x) - 1 :\n",
    "                    dataset[i][j] = float(x[j])\n",
    "                else :\n",
    "                    label[i] = float(x[j][:-1])\n",
    "    else :\n",
    "        contents =f.readlines()\n",
    "        n_data = int(len(contents)/3)\n",
    "        dataset = np.zeros((n_data,2))\n",
    "        label = np.zeros((n_data,2))\n",
    "        j = 0\n",
    "        count = 0\n",
    "        for i in range(len(contents)):\n",
    "            if(j == 1):\n",
    "                dataset[count][0] = float(contents[i].split()[0])\n",
    "                dataset[count][1] = float(contents[i].split()[1])\n",
    "            if(j == 2):\n",
    "                label[count][0] = int(contents[i].split()[0])\n",
    "                label[count][1] = int(contents[i].split()[1])\n",
    "                count = count + 1\n",
    "                j = -1\n",
    "            j += 1\n",
    "            \n",
    "    return dataset,label\n",
    "\n",
    "def norm(data_r):\n",
    "    data = data_r.copy()\n",
    "    datanorm = (data - data.min())/(data.max() - data.min()) \n",
    "    return datanorm, data.max(), data.min()\n",
    "\n",
    "def convert_norm(pred,mx,mn):\n",
    "    return pred*(mx - mn) + mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T16:19:22.257524Z",
     "start_time": "2019-10-20T16:19:22.210631Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class NN :\n",
    "    \n",
    "    def __init__(self,shape,nueral_shape,acti_funct):\n",
    "        shape[1:1] = nueral_shape \n",
    "        self.shape = shape\n",
    "        self.act_func = acti_funct\n",
    "        self.weights = self.init_weights(self.shape)\n",
    "        self.outputs = None\n",
    "        self.deltas = None\n",
    "        self.del_old_weights = None\n",
    "        \n",
    "    def init_old_weights(self,network_shape):\n",
    "        weight_arrays = []\n",
    "        for i in range(0, len(network_shape) - 1):\n",
    "            cur_idx = i\n",
    "            next_idx = i + 1\n",
    "            weight_array = np.zeros((network_shape[next_idx], network_shape[cur_idx]))\n",
    "            weight_arrays.append(weight_array)\n",
    "        \n",
    "        return weight_arrays\n",
    "    \n",
    "    def init_weights(self,network_shape):\n",
    "        weight_arrays = []\n",
    "        for i in range(0, len(network_shape) - 1):\n",
    "            cur_idx = i\n",
    "            next_idx = i + 1\n",
    "            weight_array = 2*np.random.rand(network_shape[next_idx], network_shape[cur_idx]) - 1\n",
    "            weight_arrays.append(weight_array)\n",
    "\n",
    "        return weight_arrays\n",
    "\n",
    "\n",
    "    def predict(self,sample):\n",
    "        \n",
    "        current_input =  (sample.copy()).T\n",
    "        outputs = []\n",
    "        for network_weight in self.weights:\n",
    "            current_output_temp = np.dot(network_weight, current_input)\n",
    "            current_output = self.acti_funct(current_output_temp)\n",
    "            outputs.append(current_output)\n",
    "            current_input = current_output\n",
    "        \n",
    "        if(self.shape[-1] == 1) :\n",
    "            return current_output.T\n",
    "        else :\n",
    "            tp = None \n",
    "            fp = None \n",
    "            for i in range(len(outputs[-1])):\n",
    "                if( i == 0) :\n",
    "                    tp = outputs[-1][i]\n",
    "                else :\n",
    "                    fp = outputs[-1][i]\n",
    "                    tp = np.vstack((tp, fp)).T\n",
    "            return np.argmax(tp, axis=1)\n",
    "                    \n",
    "    def train(self,sample, d_out, training_rate,momentum_rate,epoch,show=True):\n",
    "        sample_T = (sample.copy()).T\n",
    "        d_out_T = (d_out.copy()).T\n",
    "        for i in range(epoch):\n",
    "            self.FW_NN(sample_T)\n",
    "            self.BW_NN(d_out_T)\n",
    "            self.update_weights(sample_T,learning_rate,momentum_rate,i)\n",
    "            sqe = self.sum_sqaure_error(self.predict(sample),d_out_T)\n",
    "            if(show and i % 10 == 0):\n",
    "                print('Epoch : #'+str(i)+',  Sum Square Error : '+str(sqe))\n",
    "            if sqe < np.finfo(np.float32).eps :\n",
    "                break\n",
    "                \n",
    "    def FW_NN(self,input):\n",
    "\n",
    "        current_input = input\n",
    "        outputs = []\n",
    "        for w in self.weights:\n",
    "            current_output_tmp = np.dot(w, current_input)\n",
    "            current_output = self.acti_funct(current_output_tmp)\n",
    "            outputs.append(current_output)\n",
    "            current_input = current_output\n",
    "        self.outputs = outputs\n",
    "        \n",
    "    def BW_NN(self,d_out):\n",
    "        \n",
    "        deltas = []\n",
    "        O_error = d_out - self.outputs[len(self.outputs)-1]\n",
    "        O_delta = O_error *self.derivertive_acti_funct(self.outputs[len(self.outputs)-1])\n",
    "        deltas.append(O_delta)\n",
    "\n",
    "        cur_delta = O_delta\n",
    "        back_idx = len(self.outputs) - 2\n",
    "\n",
    "        for w in self.weights[::-1][:-1]:\n",
    "            hidd_error = np.dot(w.T, cur_delta)\n",
    "            hidd_delta = hidd_error * self.derivertive_acti_funct(self.outputs[back_idx])\n",
    "            deltas.append(hidd_delta)\n",
    "            cur_delta = hidd_delta\n",
    "            back_idx -= 1\n",
    "            \n",
    "        self.deltas = deltas\n",
    "    \n",
    "    def update_weights(self,sample,learning_rate,momentum_rate,count):\n",
    "        index_current_weight = len(self.weights) - 1\n",
    "        current_dels = []\n",
    "        for d in self.deltas:\n",
    "            sample_used = None\n",
    "            if index_current_weight - 1 < 0:\n",
    "                sample_used = sample\n",
    "            else:\n",
    "                sample_used = self.outputs[index_current_weight - 1]\n",
    "                \n",
    "            current_delta = learning_rate*np.dot(d, sample_used.T)\n",
    "            \n",
    "            if(count == 0) :\n",
    "                self.weights[index_current_weight] +=  current_delta\n",
    "            else :\n",
    "                self.weights[index_current_weight] +=  momentum_rate*self.del_old_weights[index_current_weight]+ current_delta\n",
    "            index_current_weight -= 1\n",
    "            current_dels.insert(0, current_delta)\n",
    "            \n",
    "        self.del_old_weights = current_dels\n",
    "\n",
    "    def acti_funct(self,v):\n",
    "        if self.act_func == 'sigmoid' :\n",
    "            return 1 / (1 + np.exp(-v))\n",
    "        if self.act_func == 'tanh' :\n",
    "            return np.tanh(v)\n",
    "        if self.act_func == 'linear' :\n",
    "            return v\n",
    "        return v\n",
    "\n",
    "    def derivertive_acti_funct(self,v):\n",
    "        if self.act_func == 'sigmoid' :\n",
    "            return v * (1 - v)\n",
    "        if self.act_func == 'tanh' :\n",
    "            return 1 - (v ** 2)\n",
    "        if self.act_func == 'linear' :\n",
    "            return 1\n",
    "        return v\n",
    "\n",
    "        \n",
    "    def sum_sqaure_error(self,pred,real):\n",
    "        real_m = real.copy()\n",
    "        sums = 0\n",
    "        if(real.ndim > 1) :\n",
    "            tp = None \n",
    "            fp = None \n",
    "            for i in range(len(real_m)):\n",
    "                if( i == 0) :\n",
    "                    tp = real_m[i]\n",
    "                else :\n",
    "                    fp = real_m[i]\n",
    "                    tp = np.vstack((tp, fp)).T\n",
    "            real_m = np.argmax(tp, axis=1)\n",
    "        for i in range(len(pred)):\n",
    "            sums = sums + np.square(pred[i]-real_m[i])\n",
    "        return sums/2\n",
    "    \n",
    "    def conf_matrix(self,pred,true,is_norm=False,confuse=True,Table=True):\n",
    "        true_m = np.zeros(len(true))\n",
    "        if(true.ndim > 1) :\n",
    "            for i in range(len(true)):\n",
    "                true_m[i] = np.argmax(true[i], axis=0)\n",
    "        if(is_norm):\n",
    "            sqr_error = 0 \n",
    "            if(Table):\n",
    "                print('Desired Output\\t\\t|\\tPredict\\t\\t\\t|\\tError')\n",
    "                print('-----------------------------------------------------------------------------')\n",
    "            for i in range(len(true)):\n",
    "                error = round(true[i] - round(pred[i][0],8),2)\n",
    "                if(Table):\n",
    "                    print(str(int(true[i]))+'\\t\\t\\t\\t|\\t'+str(format(round(pred[i][0],8), '.8f'))+'\\t\\t|\\t'+str(error))\n",
    "                sqr_error = sqr_error + (error * error)\n",
    "            if(Table):\n",
    "                print('-----------------------------------------------------------------------------')\n",
    "                print('\\t\\t Mean Square Error = '+str(round(sqr_error/len(true),6)))\n",
    "                print('=======================================')\n",
    "            return round(sqr_error/len(true),6)\n",
    "        else :\n",
    "            if(Table):\n",
    "                print('Desired Output\\t\\t|\\tPredict\\t\\t\\t')\n",
    "                print('------------------------------------------------')\n",
    "                for i in range(len(true)):\n",
    "                    print(str(int(true_m[i]))+'\\t\\t\\t\\t|\\t'+str(pred[i]))\n",
    "                print('------------------------------------------------')\n",
    "        if(confuse):\n",
    "            print('\\n\\t\\t Confusion Matrix')\n",
    "            TP = 0\n",
    "            FN = 0\n",
    "            FP = 0\n",
    "            TN = 0\n",
    "            for i in range(len(true)):\n",
    "                if((pred[i] == 0) and ( true_m[i] == 0)):\n",
    "                    TN = TN + 1 \n",
    "                elif((pred[i] == 1) and ( true_m[i] == 1)):\n",
    "                    TP = TP + 1\n",
    "                elif((pred[i] == 1) and ( true_m[i] == 0)):\n",
    "                    FP = FP + 1\n",
    "                else :\n",
    "                    FN = FN + 1\n",
    "\n",
    "            print(' ----------------------------------------')\n",
    "            for i in range(8):\n",
    "                print('|\\t\\t\\t|\\t\\t\\t|')\n",
    "                if(i == 1):\n",
    "                    print('|\\t    '+str(TN)+'\\t\\t|\\t    '+str(FP)+'\\t\\t|\\t '+str(FP+TN))\n",
    "                if(i == 3):\n",
    "                    print(' ----------------------------------------')\n",
    "                if(i == 5):\n",
    "                    print('|\\t    '+str(FN)+'\\t\\t|\\t    '+str(TP)+'\\t\\t|\\t '+str(FN+TP))\n",
    "            print(' ----------------------------------------')\n",
    "            print(' \\t    '+str(TN+FN)+'\\t\\t       '+str(FP+TP)+'\\t\\t\\t'+str(TN+FP+FN+TP))\n",
    "            print('')\n",
    "            print('Accuracy : '+str((TN+TP)/(TN+FP+FN+TP)))\n",
    "            return((TN+TP)/(TN+FP+FN+TP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T16:19:37.261961Z",
     "start_time": "2019-10-20T16:19:37.254965Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(name,cross):\n",
    "    is_norm = False\n",
    "    if(name  == 1):\n",
    "        dataset,label = load_txt(\"./Flood_dataset.txt\")\n",
    "        dataset,mx_dataset,mn_dataset = norm(dataset)\n",
    "        label,mx_label,mn_label = norm(label)\n",
    "        is_norm = True\n",
    "        max_min = [mx_dataset,mn_dataset,mx_label,mn_label]\n",
    "    else :\n",
    "        dataset,label = load_txt(\"./cross.pat\")\n",
    "    n_sample = np.arange(len(dataset))\n",
    "    np.random.shuffle(n_sample)\n",
    "    if(is_norm) :\n",
    "        return dataset,label,n_sample,max_min\n",
    "    else :\n",
    "        return dataset,label,n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T20:50:28.957137Z",
     "start_time": "2019-10-20T20:50:26.139469Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MLP(Nueral,learning_rate,momentum_rate,activation,epoch,cross_valda_train_test,data_num) :\n",
    "    if(data_num == 0):\n",
    "        print('------------------- Variable -------------------\\n')\n",
    "        dataset,label,n_sample = load_data(data_num,cross_valda_train_test)\n",
    "        data_name = 'cross.pat'\n",
    "    else :\n",
    "        print('-------------------------------- Variable --------------------------------\\n')\n",
    "        dataset,label,n_sample,max_min = load_data(data_num,cross_valda_train_test)\n",
    "        data_name = 'Flood data set'\n",
    "    n_test_per_round = int(len(dataset)*cross_valda_train_test[1]/100)\n",
    "    print('Datafile : ' +str(data_name),end='\\n')\n",
    "    print('Nueral name : '+str(len(dataset[0]))+'-',end='')\n",
    "    for i in range(len(Nueral)):\n",
    "        print(str(Nueral[i])+'-',end='')\n",
    "    print(label.ndim,end='\\n')\n",
    "    print('Learning rate : '+str(learning_rate),end='\\n')\n",
    "    print('Momentum rate : '+str(momentum_rate),end='\\n')\n",
    "    print('Activaion Function : ' +str(activation),end='\\n')\n",
    "    print('Cross validation : ['+str(cross_valda_train_test[0])+' : '+str(cross_valda_train_test[1])+']',end='\\n')\n",
    "    print('#Epoch : '+str(epoch),end='\\n')\n",
    "    error_avg = []\n",
    "    acc_avg = []\n",
    "    for i in range(10):\n",
    "        test_data = n_sample[i*n_test_per_round:i*n_test_per_round+n_test_per_round]\n",
    "        train_data = list(set(n_sample) - set(test_data))\n",
    "        nn = NN([len(dataset[0]),label.ndim],Nueral,activation)\n",
    "        nn.train(dataset[train_data],label[train_data],learning_rate,momentum_rate,epoch,False)\n",
    "        pred = nn.predict(dataset[test_data])\n",
    "        if(data_num == 1):\n",
    "#             print('\\n-------------------------------- Round : '+str(i)+' --------------------------------')\n",
    "            pred = convert_norm(pred,max_min[2],max_min[3])\n",
    "            test_label = convert_norm(label[test_data],max_min[2],max_min[3])\n",
    "            error_avg.append(nn.conf_matrix(pred,test_label,is_norm=True,confuse=False,Table=True))\n",
    "        else :\n",
    "#             print('\\n----------------- Round : '+str(i)+' -----------------')\n",
    "            acc_avg.append(nn.conf_matrix(pred,label[test_data],is_norm=False,confuse=True,Table=True))\n",
    "    if(data_num == 1):\n",
    "        print('\\n********  Mean Square Error Average : ' + str(round(np.sum(error_avg)/len(error_avg),4))+'  *********')\n",
    "        print(np.min(error_avg))\n",
    "        return round(np.sum(error_avg)/len(error_avg),4),np.min(error_avg)\n",
    "    else :\n",
    "        print('\\n********  Accuracy Average : ' + str(round(np.sum(acc_avg)/len(acc_avg),4))+'  *********')\n",
    "        return round(np.sum(acc_avg)/len(acc_avg),4),np.max(acc_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Flood_dataset.txt\n",
    "dataset, label = load_txt(\"./Flood_dataset.txt\")\n",
    "plt.plot(label, label=\"Y\")\n",
    "plt.plot(dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization cross.pat\n",
    "dataset, label = load_txt(\"./cross.pat\")\n",
    "\n",
    "setAx = []\n",
    "setAy = []\n",
    "setBx = []\n",
    "setBy = []\n",
    "\n",
    "for i in range(len(label)):\n",
    "    if label[i][0] == 0:\n",
    "        setAx.append(dataset[i][0])\n",
    "        setAy.append(dataset[i][1])\n",
    "    if label[i][0] == 1:\n",
    "        setBx.append(dataset[i][0])\n",
    "        setBy.append(dataset[i][1])\n",
    "\n",
    "plt.scatter(setAx, setAy, label='SetA')\n",
    "plt.scatter(setBx, setBy, label='SetB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T20:15:30.207203Z",
     "start_time": "2019-10-20T20:03:12.553273Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find best learning_rate for Flood_dataset.txt\n",
    "error_arr = np.zeros(100)\n",
    "minn_arr = np.zeros(100)\n",
    "for i in range(1,101):\n",
    "    Nueral = [10,6] \n",
    "    cross_valda_train_test = [90,10] # train 90 , test 10\n",
    "    data_num = 1 # 0 = cross.pat , 1 = flood data set\n",
    "    learning_rate = i*0.01\n",
    "    momentum_rate = 0.07\n",
    "    activation = 'sigmoid'\n",
    "    epoch = 1000\n",
    "    error,minn = MLP(Nueral,learning_rate,momentum_rate,activation,epoch,cross_valda_train_test,data_num)\n",
    "    error_arr[i-1] = error\n",
    "    minn_arr[i-1] = minn\n",
    "\n",
    "plt.plot(error_arr, label=\"Error Avg\")\n",
    "plt.plot(minn_arr, label=\"Min Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best learning_rate for cross.pat\n",
    "error_arr = np.zeros(100)\n",
    "minn_arr = np.zeros(100)\n",
    "for i in range(1,101):\n",
    "    Nueral = [10,6] \n",
    "    cross_valda_train_test = [90,10] # train 90 , test 10\n",
    "    data_num = 0 # 0 = cross.pat , 1 = flood data set\n",
    "    learning_rate = i*0.01\n",
    "    momentum_rate = 0.07\n",
    "    activation = 'sigmoid'\n",
    "    epoch = 1000\n",
    "    error,minn = MLP(Nueral,learning_rate,momentum_rate,activation,epoch,cross_valda_train_test,data_num)\n",
    "    error_arr[i-1] = error\n",
    "    minn_arr[i-1] = minn\n",
    "\n",
    "plt.plot(error_arr, label=\"Error Avg\")\n",
    "plt.plot(minn_arr, label=\"Min Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best momentum_rate for Flood_dataset.txt\n",
    "error_arr = np.zeros(100)\n",
    "minn_arr = np.zeros(100)\n",
    "for i in range(1,101):\n",
    "    Nueral = [10,6] \n",
    "    cross_valda_train_test = [90,10] # train 90 , test 10\n",
    "    data_num = 1 # 0 = cross.pat , 1 = flood data set\n",
    "    learning_rate = 0.12\n",
    "    momentum_rate = i*0.01\n",
    "    activation = 'sigmoid'\n",
    "    epoch = 1000\n",
    "    error,minn = MLP(Nueral,learning_rate,momentum_rate,activation,epoch,cross_valda_train_test,data_num)\n",
    "    error_arr[i-1] = error\n",
    "    minn_arr[i-1] = minn\n",
    "\n",
    "plt.plot(error_arr, label=\"Error Avg\")\n",
    "plt.plot(minn_arr, label=\"Min Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best momentum_rate for cross.pat\n",
    "error_arr = np.zeros(100)\n",
    "minn_arr = np.zeros(100)\n",
    "for i in range(1,101):\n",
    "    Nueral = [10,6] \n",
    "    cross_valda_train_test = [90,10] # train 90 , test 10\n",
    "    data_num = 0 # 0 = cross.pat , 1 = flood data set\n",
    "    learning_rate = 0.12\n",
    "    momentum_rate = i*0.01\n",
    "    activation = 'sigmoid'\n",
    "    epoch = 1000\n",
    "    error,minn = MLP(Nueral,learning_rate,momentum_rate,activation,epoch,cross_valda_train_test,data_num)\n",
    "    error_arr[i-1] = error\n",
    "    minn_arr[i-1] = minn\n",
    "\n",
    "plt.plot(error_arr, label=\"Error Avg\")\n",
    "plt.plot(minn_arr, label=\"Min Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find best epoch for Flood_dataset.txt\n",
    "error_arr = [10, 50, 100, 250, 500, 1000, 2000]\n",
    "minn_arr = [10, 50, 100, 250, 500, 1000, 2000]\n",
    "for i in error_arr:\n",
    "    temp = error_arr.copy()\n",
    "    Nueral = [10,6] \n",
    "    cross_valda_train_test = [90,10] # train 90 , test 10\n",
    "    data_num = 1 # 0 = cross.pat , 1 = flood data set\n",
    "    learning_rate = 0.12\n",
    "    momentum_rate = 0.22\n",
    "    activation = 'sigmoid'\n",
    "    epoch = i\n",
    "    error,minn = MLP(Nueral,learning_rate,momentum_rate,activation,epoch,cross_valda_train_test,data_num)\n",
    "    error_arr[temp.index(i)] = error\n",
    "    minn_arr[temp.index(i)] = minn\n",
    "\n",
    "plt.plot(error_arr, label=\"Error Avg\")\n",
    "plt.plot(minn_arr, label=\"Min Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find best epoch for cross.pat\n",
    "error_arr = [10, 50, 100, 250, 500, 1000, 2000]\n",
    "minn_arr = [10, 50, 100, 250, 500, 1000, 2000]\n",
    "for i in error_arr:\n",
    "    temp = error_arr.copy()\n",
    "    Nueral = [10,6] \n",
    "    cross_valda_train_test = [90,10] # train 90 , test 10\n",
    "    data_num = 0 # 0 = cross.pat , 1 = flood data set\n",
    "    learning_rate = 0.16\n",
    "    momentum_rate = 0.57\n",
    "    activation = 'sigmoid'\n",
    "    epoch = i\n",
    "    error,minn = MLP(Nueral,learning_rate,momentum_rate,activation,epoch,cross_valda_train_test,data_num)\n",
    "    error_arr[temp.index(i)] = error\n",
    "    minn_arr[temp.index(i)] = minn\n",
    "\n",
    "plt.plot(error_arr, label=\"Error Avg\")\n",
    "plt.plot(minn_arr, label=\"Min Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T16:26:52.886557Z",
     "start_time": "2019-10-20T16:26:06.874927Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Find best hidden_layer 1layer for Flood_dataset.txt\n",
    "error_arr = np.zeros(15)\n",
    "minn_arr = np.zeros(15)\n",
    "for i in range(1,16):\n",
    "    Nueral = [i] \n",
    "    cross_valda_train_test = [90,10] # train 90 , test 10\n",
    "    data_num = 1 # 0 = cross.pat , 1 = flood data set\n",
    "    learning_rate = 0.12\n",
    "    momentum_rate = 0.22\n",
    "    activation = 'sigmoid'\n",
    "    epoch = 1000\n",
    "    error,minn = MLP(Nueral,learning_rate,momentum_rate,activation,epoch,cross_valda_train_test,data_num)\n",
    "    error_arr[i-1] = error\n",
    "    minn_arr[i-1] = minn\n",
    "plt.plot(error_arr)\n",
    "plt.plot(minn_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T17:22:41.799171Z",
     "start_time": "2019-10-20T16:43:40.153140Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Find best hidden_layer 2layer for Flood_dataset.txt\n",
    "error_arr_2 = np.zeros(144)\n",
    "minn_arr_2 = np.zeros(144)\n",
    "for i in range(1,13):\n",
    "    for j in range(1,13):\n",
    "        Nueral = [i,j] \n",
    "        cross_valda_train_test = [90,10] # train 90 , test 10\n",
    "        data_num = 1 # 0 = cross.pat , 1 = flood data set\n",
    "        learning_rate = 0.12\n",
    "        momentum_rate = 0.22\n",
    "        activation = 'sigmoid'\n",
    "        epoch = 1000\n",
    "        error,minn = MLP(Nueral,learning_rate,momentum_rate,activation,epoch,cross_valda_train_test,data_num)\n",
    "        error_arr_2[((i-1)*12)+j-1] = error\n",
    "        minn_arr_2[((i-1)*12)+j-1] = minn\n",
    "plt.plot(error_arr_2)\n",
    "plt.plot(minn_arr_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find best hidden_layer 1layer for cross.pat\n",
    "error_arr = np.zeros(15)\n",
    "minn_arr = np.zeros(15)\n",
    "for i in range(1,16):\n",
    "    Nueral = [i] \n",
    "    cross_valda_train_test = [90,10] # train 90 , test 10\n",
    "    data_num = 0 # 0 = cross.pat , 1 = flood data set\n",
    "    learning_rate = 0.16\n",
    "    momentum_rate = 0.57\n",
    "    activation = 'sigmoid'\n",
    "    epoch = 1000\n",
    "    error,minn = MLP(Nueral,learning_rate,momentum_rate,activation,epoch,cross_valda_train_test,data_num)\n",
    "    error_arr[i-1] = error\n",
    "    minn_arr[i-1] = minn\n",
    "plt.plot(error_arr)\n",
    "plt.plot(minn_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find best hidden_layer 2layer for cross.pat\n",
    "error_arr_2 = np.zeros(144)\n",
    "minn_arr_2 = np.zeros(144)\n",
    "for i in range(1,13):\n",
    "    for j in range(1,13):\n",
    "        Nueral = [i,j] \n",
    "        cross_valda_train_test = [90,10] # train 90 , test 10\n",
    "        data_num = 0 # 0 = cross.pat , 1 = flood data set\n",
    "        learning_rate = 0.16\n",
    "        momentum_rate = 0.57\n",
    "        activation = 'sigmoid'\n",
    "        epoch = 1000\n",
    "        error,minn = MLP(Nueral,learning_rate,momentum_rate,activation,epoch,cross_valda_train_test,data_num)\n",
    "        error_arr_2[((i-1)*12)+j-1] = error\n",
    "        minn_arr_2[((i-1)*12)+j-1] = minn\n",
    "plt.plot(error_arr)\n",
    "plt.plot(minn_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "311.8px",
    "left": "1164px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}